# Copyright 2025 Ruihang Li and DeepGen Team @ Shanghai Innovation Institute

"""
UniGenBench Evaluation Module for DeepGen-RL.

This module provides UniGenBench scoring functionality for evaluating
text-to-image generation models using a VLM judge model.

Based on: https://github.com/CodeGoat24/UnifiedReward/UniGenBench

CSV Format (unified columns):
    index,prompt,sub_dims

    - index: Integer index for the prompt
    - prompt: The text prompt
    - sub_dims: JSON string with testpoints info, e.g.:
      {"Testpoints": ["Style", "World Knowledge"], "Testpoint Description": ["ink painting", "pyramids"]}

Usage:
    1. Deploy UniGenBench-EvalModel via vLLM:
       vllm serve CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1 \\
           --host localhost --port 8080 ...

    2. Set environment variable:
       export UNIGENBENCH_API_URL=http://localhost:8080

    3. Configure in eval.yaml:
       datasets:
         - name: unigenbench_en
           path: unigenbench/test_prompts_en.csv
           duplicates: 4
           scoring: unigenbench
"""

import os
import re
import ast
import json
import base64
import pandas as pd
from io import BytesIO
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
import random

from PIL import Image
import requests
from requests.adapters import HTTPAdapter
from tqdm import tqdm


# ============================================================================
# Configuration Classes
# ============================================================================

@dataclass
class UniGenBenchScoringConfig:
    """Configuration for UniGenBench scoring."""
    type: str = "unigenbench"
    # CSV path is set from the dataset's path field
    csv_path: Optional[str] = None
    # Language for judge prompt: "en" (English) or "zh" (Chinese)
    language: str = "en"

    @classmethod
    def from_string(cls, scoring_type: str) -> "UniGenBenchScoringConfig":
        """
        Create config from simple string format.

        Supports formats:
            - "unigenbench" -> defaults to English
            - "unigenbench/en" -> English
            - "unigenbench/zh" -> Chinese

        Args:
            scoring_type: Scoring type string (e.g., "unigenbench", "unigenbench/en", "unigenbench/zh")

        Returns:
            UniGenBenchScoringConfig instance
        """
        # Parse language from scoring type (e.g., "unigenbench/en" -> "en")
        if "/" in scoring_type:
            parts = scoring_type.split("/")
            base_type = parts[0]
            language = parts[1] if len(parts) > 1 else "en"
        else:
            base_type = scoring_type
            language = "en"  # Default to English

        return cls(type=base_type, language=language)


# ============================================================================
# VLM Client for API Calls
# ============================================================================

# Default model name for UniGenBench evaluation
# Can be overridden via UNIGENBENCH_MODEL_NAME environment variable
DEFAULT_UNIGENBENCH_MODEL_NAME = "UniGenBench-EvalModel-qwen3vl-32b-v1"


class VLMJudgeClient:
    """
    Client for calling VLM judge model via vLLM API.

    Handles image encoding, request batching, retries, and response parsing.
    """

    def __init__(
        self,
        api_url: str,
        model_name: Optional[str] = None,
        timeout_base: int = 120,
        max_retries: int = 10,
        backoff_base: float = 2.0,
        backoff_cap: float = 30.0,
        pool_maxsize: int = 16,
    ):
        self.api_url = api_url
        # Get model name from parameter, env var, or default
        self.model_name = model_name or os.environ.get(
            "UNIGENBENCH_MODEL_NAME", DEFAULT_UNIGENBENCH_MODEL_NAME
        )
        self.timeout_base = timeout_base
        self.max_retries = max_retries
        self.backoff_base = backoff_base
        self.backoff_cap = backoff_cap
        self.pool_maxsize = pool_maxsize
        self._local = threading.local()

    def _get_session(self) -> requests.Session:
        """Get thread-local session with connection pooling."""
        session = getattr(self._local, "session", None)
        if session is None:
            session = requests.Session()
            adapter = HTTPAdapter(
                pool_connections=self.pool_maxsize,
                pool_maxsize=self.pool_maxsize,
            )
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            self._local.session = session
        return session

    def _encode_image(self, image_path: str) -> str:
        """Encode image file to base64 string."""
        with Image.open(image_path) as img:
            img = img.convert("RGB")
            buffered = BytesIO()
            img.save(buffered, format="JPEG", quality=95)
            return base64.b64encode(buffered.getvalue()).decode("utf-8")

    def _build_messages(self, image_path: str, system_prompt: str) -> List[Dict]:
        """Build chat messages for VLM API."""
        base64_image = self._encode_image(image_path)
        image_url = f"data:image/jpeg;base64,{base64_image}"

        return [
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": system_prompt},
                ],
            }
        ]

    # English explanation dictionary for testpoints
    EXPLANATION_DICT_EN = {
        "Relationship - Comparison": "Comparison of attributes between two entities",
        "Relationship - Composition": "An entity is composed of one or more other entities",
        "Relationship - Inclusion": "A container contains an entity; the container can also be a plane, e.g., a snake in a painting on a wall",
        "Relationship - Similarity": "Existence of similarities between different entities",
        "Compound - Imagination": "Things that are impossible in real life",
        "Compound - Feature Matching": "Different entities possess different types of attribute features",
        "Attribute - Size": "Assessment of the subject's size, height, length, thickness, width, or tallness/shortness",
        "Attribute - Expression": "Distinguishing expressions from facial actions; expressions must convey a clear emotion",
        "Attribute - Quantity": "Focuses on the challenge of depicting three or more items accurately",
        "Attribute - Material": "Evaluation of different material types and textures",
        "Attribute - Color": "Assessment of different colors",
        "Attribute - Shape": "Assessment of different shapes",
        "Entity Layout - Two-Dimensional Space": "Arrangement and positioning of entities in two-dimensional space",
        "Entity Layout - Three-Dimensional Space": "Arrangement and positioning of entities in three-dimensional space",
        "Action - Full-body (Character/Anthropomorphic)": "Full-body actions by characters or anthropomorphized entities, such as running, diving, breakdancing, swinging, or hanging upside down",
        "Action - Hand (Character/Anthropomorphic)": "Focuses on hand structureâ€”checking if fingers are missing, broken, or distorted",
        "Action - Animal": "Actions performed by animals",
        "Action - Contact Interaction": "Physical interactions between entities",
        "Action - Non-contact Interaction": "For example, two people making eye contactâ€”testing if the model can accurately depict such interactions",
        "Action - State": "A sustained state of an entity, typically expressed with a verb",
        "Grammar - Negation": "Tests the model's understanding of negation grammar",
        "Grammar - Pronoun Reference": "Tests if the model can resolve ambiguous pronoun references correctly",
        "Grammar - Consistency": "Evaluation of shared attributes among entities",
        "World Knowledge": "Covers knowledge of celebrities, architecture, basic domain knowledge, and internet slang. Celebrities with modern copyright risk should be avoided",
        "Style": "Art, painting, photography, design styles, and corresponding artist names",
        "Text Generation": "The text content model needed to accurately generate without any omissions or extra words",
        "Text Generation (Case-insensitive)": "The text content model needed to accurately generate without any omissions or extra words, but ignore the case of the text - which means even the generated text is not in the same case as the prompt while the spelling is correct, it should still be considered as a correct answer",
        "Logical Reasoning": "Requires the model to deeply understand the intent and perform reasoning",
    }

    # Chinese explanation dictionary for testpoints
    EXPLANATION_DICT_ZH = {
        'å…³ç³»-æ¯”è¾ƒå…³ç³»': 'ä¸¤è€…çš„å±æ€§å¯¹æ¯”',
        'å…³ç³»-æ„æˆå…³ç³»': 'ä¸€ä¸ªå®ä½“ç”±å¦ä¸€ç§æˆ–å‡ ç§å®ä½“æ„æˆ',
        'å…³ç³»-åŒ…å«å…³ç³»': 'å®¹å™¨å¯¹å®ä½“çš„åŒ…å«å…³ç³»ï¼Œå®¹å™¨ä¹Ÿå¯ä»¥æ˜¯å¹³é¢çš„ï¼Œæ¯”å¦‚ï¼šå¢™ä¸Šçš„ç”»é‡Œæœ‰ä¸€åªè›‡',
        'å…³ç³»-ç›¸ä¼¼å…³ç³»': 'ä¸åŒå®ä½“ä¸­å­˜åœ¨çš„ç›¸ä¼¼å…³ç³»',
        'å¤åˆè€ƒç‚¹-æƒ³è±¡åŠ›': 'ç°å®ç”Ÿæ´»ä¸­ä¸å¯èƒ½å‘ç”Ÿçš„äº‹æƒ…',
        'å¤åˆè€ƒç‚¹-ä¸åŒå®ä½“ç‰¹å¾åŒ¹é…': 'ä¸åŒå®ä½“æ‹¥æœ‰ä¸åŒç±»çš„å±æ€§ç‰¹å¾',
        'å®ä½“å¸ƒå±€-ä¸‰ç»´ç©ºé—´': 'å¯¹äºä¸‰ç»´ç©ºé—´å®ä½“çš„æ‘†æ”¾å¸ƒå±€',
        'å®ä½“å¸ƒå±€-äºŒç»´ç©ºé—´': 'å¯¹äºäºŒç»´ç©ºé—´å®ä½“çš„æ‘†æ”¾å¸ƒå±€',
        'å±æ€§-å¤§å°': 'å¯¹ä¸»ä½“ å¤§å°/é«˜ä½/é•¿çŸ­/ç²—ç»†/å®½çª„/é«˜çŸ®',
        'å±æ€§-è¡¨æƒ…': 'åŒºåˆ†è¡¨æƒ…å’Œè„¸éƒ¨åŠ¨ä½œï¼Œè„¸éƒ¨åŠ¨ä½œç»„æˆè¡¨æƒ…ï¼Œä½†è¡¨æƒ…æ˜¯ä¸€å®šè¦ä½“ç°å‡ºæŸç§æƒ…ç»ªçš„ã€‚',
        'å±æ€§-æ•°é‡': 'é‡ç‚¹è€ƒå¯Ÿä¸‰ä¸ªæˆ–ä¸‰ä¸ªä»¥ä¸Šçš„æ•°å­—éš¾ç‚¹',
        'å±æ€§-æè´¨': 'è€ƒå¯Ÿä¸åŒæè´¨',
        'åŠ¨ä½œ-äººç‰©/æ‹Ÿäººå…¨èº«åŠ¨ä½œ': 'äººç‰©æˆ–æ‹Ÿäººå…¨èº«æ€§çš„åŠ¨ä½œï¼Œæ¯”å¦‚å¥”è·‘ã€è·³æ°´ã€è·³è¡—èˆã€è¡ç§‹åƒã€å€’æŒ‚é‡‘é’©ç­‰',
        'åŠ¨ä½œ-äººç‰©/æ‹Ÿäººæ‰‹éƒ¨åŠ¨ä½œ': 'é’ˆå¯¹æ‰‹éƒ¨ç»“æ„çš„è€ƒç‚¹ï¼Œè€ƒæ ¸æ‰‹æŒ‡æ˜¯å¦æœ‰ç¼ºå¤±ã€å´©åç­‰é—®é¢˜',
        'åŠ¨ä½œ-åŠ¨ç‰©åŠ¨ä½œ': 'åŠ¨ç‰©çš„åŠ¨ä½œ',
        'åŠ¨ä½œ-å®ä½“é—´æœ‰æ¥è§¦äº’åŠ¨': 'å„ç§å®ä½“é—´çš„æœ‰æ¥è§¦äº’åŠ¨',
        'åŠ¨ä½œ-å®ä½“é—´æ— æ¥è§¦äº’åŠ¨': 'æ¯”å¦‚ä¸¤ä¸ªäººå¯¹è§†ï¼Œè€ƒæ ¸æ¨¡å‹èƒ½å¦æŠŠå¯¹è§†å…³ç³»ç”»å¯¹',
        'åŠ¨ä½œ-çŠ¶æ€': 'å®ä½“æŒç»­çš„çŠ¶æ€ï¼Œä¸€èˆ¬æ˜¯ä¸€ä¸ªåŠ¨è¯ã€‚',
        'è¯­æ³•-å¦å®š': 'è€ƒå¯Ÿæ¨¡å‹å¯¹äºå¦å®šè¯­æ³•çš„æŒæ¡ç¨‹åº¦',
        'è¯­æ³•-ä»£è¯æŒ‡ä»£': 'è¿™é‡Œçš„ä»£è¯é€šå¸¸æ˜¯æœ‰ä¸€äº›è¿·æƒ‘æ€§çš„ï¼Œè€ƒå¯Ÿæ¨¡å‹èƒ½å¦æ­£ç¡®å¯¹åº”',
        'è¯­æ³•-ç»Ÿä¸€æ€§': 'å®ä½“å…±åŒå±æ€§çš„è€ƒå¯Ÿ',
        'ä¸–ç•ŒçŸ¥è¯†': 'åäººã€å»ºç­‘ã€åŸºç¡€çš„é¢†åŸŸçŸ¥è¯†ã€ç½‘ç»œæµè¡Œè¯­ã€‚å…¶ä¸­åäººä¸è¦ä½¿ç”¨å½“ä»£æœ‰ç‰ˆæƒé£é™©çš„åäºº',
        'é£æ ¼': 'è‰ºæœ¯ã€ç»˜ç”»ã€æ‘„å½±ã€è®¾è®¡é£æ ¼ï¼ŒåŠå¯¹åº”è‰ºæœ¯å®¶åç§°',
        'é€»è¾‘æ¨ç†': 'éœ€è¦æ¨¡å‹æ·±å…¥ç†è§£æ„å›¾å¹¶è¿›è¡Œä¸€å®šçš„æ¨ç†',
        'æ–‡æœ¬ç”Ÿæˆ': 'è€ƒå¯Ÿæ¨¡å‹èƒ½å¦å‡†ç¡®ç”Ÿæˆä¸åŒè¯­è¨€ï¼Œå­—ä½“å’Œé•¿ã€çŸ­æ–‡å­—',
    }

    def evaluate_single(
        self,
        image_path: str,
        prompt: str,
        testpoints: List[str],
        testpoint_descriptions: List[str],
        language: str = "en",
    ) -> Dict[str, Any]:
        """
        Evaluate a single image using the VLM judge.

        Args:
            image_path: Path to the image file
            prompt: The generation prompt
            testpoints: List of testpoint names
            testpoint_descriptions: List of testpoint descriptions
            language: Language for judge prompt ("en" or "zh")

        Returns:
            Dict with evaluation results
        """
        # Select explanation dictionary based on language
        explanation_dict = self.EXPLANATION_DICT_ZH if language == "zh" else self.EXPLANATION_DICT_EN

        # Build explanation and system prompt based on language
        if language == "zh":
            # Chinese version
            explanation = "è€ƒç‚¹è¯´æ˜ï¼šã€Œ"
            for point in testpoints:
                if point in explanation_dict:
                    explanation += f"\n{point}: {explanation_dict[point]}"
                else:
                    explanation += f"\n{point}: (æ— å®šä¹‰)"
            explanation += "\nã€"

            test_explanation = "è€ƒç‚¹æè¿°è¯´æ˜ï¼šã€Œ"
            for idx, point in enumerate(testpoints):
                desc = testpoint_descriptions[idx] if idx < len(testpoint_descriptions) else ""
                test_explanation += f"\n{point}: {desc}"
            test_explanation += "\nã€"

            system_prompt = f'''ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®ä¸”å®¢è§‚çš„ä¸­æ–‡å›¾åƒæè¿°ç³»ç»Ÿã€‚æˆ‘ä¼šç»™ä½ ä¸€æ®µç”Ÿæˆå›¾åƒçš„æç¤ºè¯ï¼Œä»¥åŠå¯¹åº”çš„ç”Ÿæˆå›¾åƒï¼ŒåŒæ—¶å¯¹äºç”Ÿæˆå›¾åƒä¸æç¤ºè¯ä¹‹é—´ç›¸å…³æ€§çš„è€ƒç‚¹åŠå¯¹åº”è¯´æ˜ï¼Œä½ éœ€è¦é€ä¸ªè€ƒç‚¹æ¥åˆ¤æ–­ç”Ÿæˆçš„å›¾åƒæ˜¯å¦éµä»äº†æç¤ºè¯ä¸­æ‰€åŒ…å«çš„å¯¹åº”è€ƒç‚¹è¦æ±‚ã€‚

é’ˆå¯¹æ¯å¼ å›¾åƒï¼Œä½ éœ€è¦æŒ‰ç…§é¡ºåºå®Œæˆå¦‚ä¸‹çš„ä»»åŠ¡ï¼š
1. è¿™å¼ ç”Ÿæˆå›¾åƒå¯¹åº”çš„æç¤ºè¯ä¸ºã€Œ{prompt}ã€ï¼Œä½ éœ€è¦æ ¹æ®{testpoints}ä¸­çš„è¿™äº›è§’åº¦é€ä¸ªå¯¹å›¾åƒå†…å®¹è¿›è¡Œæ›´è¿›ä¸€æ­¥çš„è¯¦ç»†åˆ†æï¼Œè€ƒç‚¹çš„è¯¦ç»†è¯´æ˜å¦‚ä¸‹ï¼š{explanation}ï¼Œå„ä¸ªè€ƒç‚¹åœ¨è¿™æ¡promptä¸­å¯¹åº”çš„æè¿°è¯´æ˜å¦‚ä¸‹ï¼š{test_explanation}, ä½ éœ€è¦æ ¹æ®è€ƒç‚¹é€ä¸€åˆ¤æ–­ç”Ÿæˆå›¾åƒæ˜¯å¦æ»¡è¶³äº†è€ƒç‚¹å¯¹åº”çš„è¦æ±‚
2. ç»¼åˆä¸Šè¿°å›ç­”ï¼Œä½ éœ€è¦é€ä¸ªè€ƒç‚¹åˆ¤æ–­ç”Ÿæˆçš„å›¾åƒåœ¨è€ƒç‚¹å…³æ³¨ç»´åº¦ä¸Šæ˜¯å¦ç¬¦åˆè¾“å…¥çš„promptï¼Œå¦‚æœæ»¡è¶³è¦æ±‚åˆ™è¯¥è€ƒç‚¹å¾—åˆ†ä¸º1ï¼Œå¦åˆ™ä¸º0

çº¦æŸæ¡ä»¶ï¼š
- ä»…æè¿°ç›´æ¥å¯è§çš„å†…å®¹ï¼›ä¸è¦è¿›è¡Œè§£è¯»ã€æ¨æµ‹æˆ–æš—ç¤ºèƒŒæ™¯æ•…äº‹ã€‚
- ä¸“æ³¨äºèƒ½å¤Ÿç¡®å®šæ€§é™ˆè¿°çš„è§†è§‰ç»†èŠ‚ã€‚
- çœç•¥ä¸ç¡®å®šæˆ–ä¸æ¸…æ™°çš„ç»†èŠ‚ã€‚
- å³ä½¿è¾“å…¥ä¸­å­˜åœ¨ï¼Œä¹Ÿä¸è¦æè¿°æŠ½è±¡å®ä½“ã€æƒ…æ„Ÿæˆ–æ¨æµ‹ã€‚

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¾“å‡ºæ ¼å¼ï¼š

<description>
    <prompt>{prompt}</prompt>
    <checkpoint>{testpoints}</checkpoint>
    <analysis>æŒ‰ç…§æ­¥éª¤1å¯¹äºç»™å®šè€ƒç‚¹è¿›è¡Œé€é¡¹è¯¦ç»†åˆ†æï¼Œæ ¼å¼ä¸ºä¸€ä¸ªæ–¹æ‹¬å·åˆ—è¡¨ï¼Œ**ç¡®ä¿åˆ—è¡¨çš„é•¿åº¦ä¸è€ƒç‚¹çš„æ•°é‡ç›¸ç­‰**ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºå¯¹äºå¯¹åº”è€ƒç‚¹çš„åˆ†æ</analysis>
    <score>æŒ‰ç…§æ­¥éª¤2é€ä¸ªå¯¹è€ƒç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œæ ¼å¼ä¸ºä¸€ä¸ªæ–¹æ‹¬å·åˆ—è¡¨ï¼Œ**ç¡®ä¿åˆ—è¡¨çš„é•¿åº¦ä¸è€ƒç‚¹çš„æ•°é‡ç›¸ç­‰**ï¼Œæ¯ä¸ªå…ƒç´ ä¸º0æˆ–è€…1ï¼Œè¡¨ç¤ºå¯¹åº”è€ƒç‚¹æ˜¯å¦å®Œæˆ</score>
</description>
'''
        else:
            # English version (default)
            explanation = "Checkpoints Definition:ã€Œ"
            for point in testpoints:
                if point in explanation_dict:
                    explanation += f"\n{point}: {explanation_dict[point]}"
                else:
                    explanation += f"\n{point}: (No definition available)"
            explanation += "\nã€"

            test_explanation = "Checkpoints Description:ã€Œ"
            for idx, point in enumerate(testpoints):
                desc = testpoint_descriptions[idx] if idx < len(testpoint_descriptions) else ""
                test_explanation += f"\n{point}: {desc}"
            test_explanation += "\nã€"

            system_prompt = f'''You are a precise and objective English-language image description system. I will provide you with a prompt for image generation, as well as the corresponding generated image. You will be given a set of evaluation criteria (checkpoints) and their explanations that define the relevance between the prompt and the image. You must evaluate whether the generated image fulfills the requirements implied by each checkpoint in the prompt.

For each image, follow the steps below in order:

1. The prompt for the generated image is: ã€Œ{prompt}ã€. You are to analyze the image content in detail from the angles specified in {testpoints}. Detailed definitions of these checkpoints are provided here: {explanation}. The specific description of each checkpoint in the context of the prompt is: {test_explanation}. You must analyze whether the image meets the requirements for each checkpoint individually.

2. Based on the above analysis, determine whether the generated image satisfies each checkpoint in terms of its visual alignment with the prompt. If the image meets the requirements of a checkpoint, assign a score of 1 to that checkpoint; otherwise, assign a score of 0.

Constraints:
- Only describe content that is directly visible; do not interpret, speculate, or infer any background story.
- Focus solely on visually verifiable details.
- Omit any uncertain or ambiguous elements.
- Even if mentioned in the input, do not describe abstract entities, emotions, or speculative ideas.

Please strictly follow the output format below:

<description>
    <prompt>{prompt}</prompt>
    <checkpoint>{testpoints}</checkpoint>
    <analysis>A list using square brackets `[]`, where each element is a string of detailed analysis corresponding to one checkpoint, as required in Step 1. **Ensure the list length matches the number of checkpoints**. Each element should be a string representing the analysis for that specific checkpoint.</analysis>
    <score>A list using square brackets `[]`, where each element is a binary score (0 or 1) corresponding to a checkpoint, as required in Step 2. **Ensure the list length matches the number of checkpoints**. Each element should be either 0 or 1, indicating whether the checkpoint was satisfied.</score>
</description>
'''

        # Call VLM API with retries
        attempt = 0
        last_error = None

        while attempt < self.max_retries:
            try:
                attempt += 1
                session = self._get_session()
                messages = self._build_messages(image_path, system_prompt)

                payload = {
                    "model": self.model_name,
                    "messages": messages,
                    "do_sample": False,
                    "max_tokens": 4096,
                }

                response = session.post(
                    f"{self.api_url}/v1/chat/completions",
                    json=payload,
                    timeout=self.timeout_base + attempt * 10,
                )

                if response.status_code in {429, 500, 502, 503, 504}:
                    raise requests.HTTPError(f"Retryable HTTP {response.status_code}")

                response.raise_for_status()
                output = response.json()["choices"][0]["message"]["content"]

                # Parse response
                return self._parse_response(output, testpoints, prompt, image_path)

            except Exception as e:
                last_error = str(e)
                if attempt < self.max_retries:
                    sleep_time = min(
                        self.backoff_base ** attempt + random.uniform(0, 1),
                        self.backoff_cap,
                    )
                    time.sleep(sleep_time)

        # All retries failed
        return {
            "success": False,
            "error": last_error,
            "prompt": prompt,
            "image_path": image_path,
            "testpoints": testpoints,
            "scores": [0] * len(testpoints),  # Default to 0 on failure
        }

    def _parse_response(
        self,
        text: str,
        testpoints: List[str],
        prompt: str,
        image_path: str,
    ) -> Dict[str, Any]:
        """Parse VLM response to extract scores."""
        try:
            analysis_match = re.search(r'<analysis>(.*?)</analysis>', text, re.DOTALL)
            score_match = re.search(r'<score>(.*?)</score>', text, re.DOTALL)

            if not analysis_match or not score_match:
                return {
                    "success": False,
                    "error": "Could not parse analysis/score tags",
                    "raw_output": text,
                    "prompt": prompt,
                    "image_path": image_path,
                    "testpoints": testpoints,
                    "scores": [0] * len(testpoints),
                }

            analysis_str = analysis_match.group(1).strip()
            score_str = score_match.group(1).strip()

            analysis = ast.literal_eval(analysis_str)
            scores = ast.literal_eval(score_str)

            # Validate lengths match
            if len(scores) != len(testpoints):
                return {
                    "success": False,
                    "error": f"Score count mismatch: {len(scores)} vs {len(testpoints)}",
                    "raw_output": text,
                    "prompt": prompt,
                    "image_path": image_path,
                    "testpoints": testpoints,
                    "scores": [0] * len(testpoints),
                }

            return {
                "success": True,
                "prompt": prompt,
                "image_path": image_path,
                "testpoints": testpoints,
                "scores": scores,
                "analysis": analysis,
                "raw_output": text,
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "raw_output": text,
                "prompt": prompt,
                "image_path": image_path,
                "testpoints": testpoints,
                "scores": [0] * len(testpoints),
            }


# ============================================================================
# Main Scorer Class
# ============================================================================

class UniGenBenchScorer:
    """
    UniGenBench scorer for evaluating text-to-image generation.

    Loads test prompts with testpoints from CSV, evaluates generated images
    using a VLM judge, and computes accuracy across dimensions.
    """

    def __init__(
        self,
        csv_path: str,
        api_url: Optional[str] = None,
        max_workers: Optional[int] = None,
        language: str = "en",
    ):
        """
        Initialize the scorer.

        Args:
            csv_path: Path to CSV file with columns: index, prompt, sub_dims
            api_url: VLM API URL (defaults to UNIGENBENCH_API_URL env var)
            max_workers: Max concurrent workers for API calls
            language: Language for judge prompt ("en" or "zh")
        """
        self.csv_path = csv_path
        self.language = language

        # Get API URL from env if not provided
        self.api_url = api_url or os.environ.get("UNIGENBENCH_API_URL")
        if not self.api_url:
            raise ValueError(
                "UniGenBench API URL not configured. "
                "Set UNIGENBENCH_API_URL environment variable or pass api_url parameter."
            )

        # Get max workers from env or default
        self.max_workers = max_workers or int(os.environ.get("UNIGENBENCH_WORKERS", "16"))

        # Initialize VLM client
        self.client = VLMJudgeClient(api_url=self.api_url, pool_maxsize=self.max_workers)

        # Log configuration
        print(f"[UniGenBench] API URL: {self.api_url}")
        print(f"[UniGenBench] Model Name: {self.client.model_name}")
        print(f"[UniGenBench] Max Workers: {self.max_workers}")
        print(f"[UniGenBench] Language: {self.language}")

        # Load test prompts CSV
        self.prompts_data: Dict[int, Dict[str, Any]] = {}
        self._load_prompts_csv()

    def _load_prompts_csv(self) -> None:
        """
        Load test prompts CSV with unified column names.

        Expected columns: index, prompt, sub_dims
        """
        if not os.path.exists(self.csv_path):
            raise FileNotFoundError(f"Test prompts CSV not found: {self.csv_path}")

        df = pd.read_csv(self.csv_path)

        # Validate required columns
        required_cols = ["index", "prompt", "sub_dims"]
        missing_cols = [c for c in required_cols if c not in df.columns]
        if missing_cols:
            raise ValueError(
                f"CSV missing required columns: {missing_cols}. "
                f"Expected columns: {required_cols}. "
                f"Found columns: {list(df.columns)}"
            )

        for _, row in df.iterrows():
            index = int(row["index"])
            prompt = row["prompt"]

            # Parse sub_dims JSON
            subdims_str = row["sub_dims"]
            try:
                subdims = json.loads(subdims_str) if isinstance(subdims_str, str) else subdims_str
            except json.JSONDecodeError:
                subdims = {}

            testpoints = subdims.get("Testpoints", [])
            testpoint_desc = subdims.get("Testpoint Description", [])

            self.prompts_data[index] = {
                "prompt": prompt,
                "testpoints": testpoints,
                "testpoint_description": testpoint_desc,
                "subdims": subdims,
            }

        print(f"[UniGenBench] Loaded {len(self.prompts_data)} prompts from {self.csv_path}")

    def score_images(
        self,
        image_dir: str,
        num_duplicates: int = 4,
        show_progress: bool = True,
    ) -> Dict[str, Any]:
        """
        Score all images in a directory.

        Args:
            image_dir: Directory containing generated images
            num_duplicates: Number of images per prompt (default: 4)
            show_progress: Whether to show progress bar

        Returns:
            Dict with scoring results including per-dimension accuracy
        """
        # Collect all evaluation tasks
        tasks = []
        for index, data in self.prompts_data.items():
            for dup_idx in range(num_duplicates):
                # Image naming convention: {index}.{dup_idx}.png
                img_filename = f"{index}.{dup_idx}.png"
                img_path = os.path.join(image_dir, img_filename)

                if os.path.exists(img_path):
                    tasks.append({
                        "index": index,
                        "dup_idx": dup_idx,
                        "image_path": img_path,
                        "prompt": data["prompt"],
                        "testpoints": data["testpoints"],
                        "testpoint_description": data["testpoint_description"],
                    })

        if not tasks:
            print(f"[UniGenBench] Warning: No images found in {image_dir}")
            return {"success": False, "error": "No images found"}

        print(f"[UniGenBench] Scoring {len(tasks)} images...")

        # Execute evaluations in parallel
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(
                    self.client.evaluate_single,
                    task["image_path"],
                    task["prompt"],
                    task["testpoints"],
                    task["testpoint_description"],
                    self.language,  # Pass language parameter
                ): task
                for task in tasks
            }

            progress_iter = tqdm(
                as_completed(futures),
                total=len(futures),
                desc="[UniGenBench] Evaluating",
                disable=not show_progress,
            )

            for future in progress_iter:
                task = futures[future]
                try:
                    result = future.result()
                    result["index"] = task["index"]
                    result["dup_idx"] = task["dup_idx"]
                    results.append(result)
                except Exception as e:
                    results.append({
                        "success": False,
                        "error": str(e),
                        "index": task["index"],
                        "dup_idx": task["dup_idx"],
                        "testpoints": task["testpoints"],
                        "scores": [0] * len(task["testpoints"]),
                    })

        # Compute statistics
        return self._compute_statistics(results)

    def _compute_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Compute accuracy statistics from evaluation results.

        Returns:
            Dict with:
            - overall_accuracy: Overall accuracy across all testpoints
            - primary_dims: Dict of primary dimension accuracies
            - sub_dims: Dict of sub-dimension accuracies
            - results_csv: List of results suitable for saving to CSV
        """
        primary_stats = defaultdict(lambda: {"correct": 0, "total": 0})
        sub_stats = defaultdict(lambda: {"correct": 0, "total": 0})

        success_count = 0
        results_csv = []

        for result in results:
            if result.get("success", False):
                success_count += 1

            testpoints = result.get("testpoints", [])
            scores = result.get("scores", [])

            # Record for CSV
            results_csv.append({
                "index": result.get("index"),
                "dup_idx": result.get("dup_idx"),
                "success": result.get("success", False),
                "testpoints": str(testpoints),
                "scores": str(scores),
                "raw_output": result.get("raw_output", ""),
            })

            # Aggregate statistics
            for cp, score in zip(testpoints, scores):
                # Determine primary and sub dimension
                if " - " in cp:
                    primary = cp.split(" - ", 1)[0].strip()
                    sub = cp
                else:
                    primary = cp
                    sub = cp

                primary_stats[primary]["total"] += 1
                sub_stats[sub]["total"] += 1

                if score == 1:
                    primary_stats[primary]["correct"] += 1
                    sub_stats[sub]["correct"] += 1

        # Compute accuracies
        primary_acc = {}
        for dim, stats in primary_stats.items():
            acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            primary_acc[dim] = {
                "accuracy": acc,
                "correct": stats["correct"],
                "total": stats["total"],
            }

        sub_acc = {}
        for dim, stats in sub_stats.items():
            acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            sub_acc[dim] = {
                "accuracy": acc,
                "correct": stats["correct"],
                "total": stats["total"],
            }

        # Compute overall accuracy
        total_correct = sum(s["correct"] for s in sub_stats.values())
        total_count = sum(s["total"] for s in sub_stats.values())
        overall_acc = total_correct / total_count if total_count > 0 else 0.0

        return {
            "success": True,
            "overall_accuracy": overall_acc,
            "total_correct": total_correct,
            "total_count": total_count,
            "success_rate": success_count / len(results) if results else 0.0,
            "primary_dims": primary_acc,
            "sub_dims": sub_acc,
            "results_csv": results_csv,
        }

    def format_wandb_metrics(
        self,
        stats: Dict[str, Any],
        prefix: str = "eval/unigenbench",
    ) -> Dict[str, float]:
        """
        Format statistics for wandb logging.

        Args:
            stats: Statistics from score_images()
            prefix: Metric name prefix

        Returns:
            Dict of metric_name -> value for wandb.log()

        Metric format examples:
            - eval/unigenbench_en/overall_accuracy
            - eval/unigenbench_en/Style
            - eval/unigenbench_en/Attribute.Attribute-Quantity
        """
        metrics = {}

        # Overall accuracy
        metrics[f"{prefix}/overall_accuracy"] = stats.get("overall_accuracy", 0.0)
        metrics[f"{prefix}/success_rate"] = stats.get("success_rate", 0.0)

        # Primary and sub dimension accuracies
        # Format: eval/<dataset_name>/<Primary>.<Sub> where Sub uses "-" instead of " - "
        for sub_dim, data in stats.get("sub_dims", {}).items():
            # Determine primary dimension
            if " - " in sub_dim:
                primary = sub_dim.split(" - ", 1)[0].strip()
                # Convert "Attribute - Quantity" to "Attribute-Quantity"
                sub_clean = sub_dim.replace(" - ", "-")
            else:
                primary = sub_dim
                sub_clean = sub_dim

            # Format: eval/unigenbench_en/Primary.Sub-Dim
            if primary != sub_dim:
                metric_name = f"{prefix}/{primary}.{sub_clean}"
            else:
                metric_name = f"{prefix}/{sub_clean}"

            metrics[metric_name] = data["accuracy"]

        return metrics

    def print_results(self, stats: Dict[str, Any]) -> None:
        """Print formatted evaluation results."""
        print("\n" + "=" * 70)
        print("UniGenBench Evaluation Results")
        print("=" * 70)

        print(f"\nOverall Accuracy: {stats['overall_accuracy']:.2%}")
        print(f"Success Rate: {stats['success_rate']:.2%}")
        print(f"Total Testpoints: {stats['total_count']}")

        print("\nğŸ“˜ Primary Dimension Results:")
        for dim, data in sorted(stats.get("primary_dims", {}).items()):
            print(f"  - {dim}: {data['correct']}/{data['total']} = {data['accuracy']:.2%}")

        print("\nğŸ“— Sub Dimension Results:")
        for dim, data in sorted(stats.get("sub_dims", {}).items()):
            print(f"  - {dim}: {data['correct']}/{data['total']} = {data['accuracy']:.2%}")

        print("=" * 70 + "\n")


# ============================================================================
# Utility Functions
# ============================================================================

def is_unigenbench_enabled() -> bool:
    """Check if UniGenBench evaluation is enabled (API URL configured)."""
    return os.environ.get("UNIGENBENCH_API_URL") is not None


def parse_scoring_config(
    scoring_value: Any,
    config_dir: str,
) -> Optional[UniGenBenchScoringConfig]:
    """
    Parse scoring configuration from eval.yaml.

    Supports multiple formats:
        - scoring: unigenbench          # defaults to English
        - scoring: unigenbench/en       # English
        - scoring: unigenbench/zh       # Chinese
        - scoring:
            type: unigenbench
            language: zh

    Args:
        scoring_value: Scoring config value from YAML (string or dict)
        config_dir: Directory of eval.yaml for resolving relative paths

    Returns:
        UniGenBenchScoringConfig instance or None if not unigenbench type
    """
    if scoring_value is None:
        return None

    # Simple string format: scoring: unigenbench or unigenbench/en or unigenbench/zh
    if isinstance(scoring_value, str):
        # Check if it starts with "unigenbench"
        if scoring_value.startswith("unigenbench"):
            return UniGenBenchScoringConfig.from_string(scoring_value)
        return None

    # Dict format: scoring: {type: unigenbench, language: zh, ...}
    if isinstance(scoring_value, dict):
        scoring_type = scoring_value.get("type", "")
        if scoring_type == "unigenbench" or scoring_type.startswith("unigenbench/"):
            language = scoring_value.get("language", "en")
            # Also support type: unigenbench/zh format
            if "/" in scoring_type:
                language = scoring_type.split("/")[1]
            return UniGenBenchScoringConfig(type="unigenbench", language=language)
        return None

    # List format (legacy): scoring: [{type: unigenbench, ...}]
    if isinstance(scoring_value, list):
        for item in scoring_value:
            if isinstance(item, dict):
                item_type = item.get("type", "")
                if item_type == "unigenbench" or item_type.startswith("unigenbench/"):
                    language = item.get("language", "en")
                    if "/" in item_type:
                        language = item_type.split("/")[1]
                    return UniGenBenchScoringConfig(type="unigenbench", language=language)
        return None

    return None
